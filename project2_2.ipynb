{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "name": ""
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "3a1849b7-7dfe-4926-a83d-03640bee6035",
      "cell_type": "markdown",
      "source": "# Wstęp do Uczenia Maszynowego - Projekt 2\n## Etap: Drugi Kamień Milowy \n### Autorzy: Krzysztof Osiński, Jakub Miszczak",
      "metadata": {}
    },
    {
      "id": "a7670d52-cfff-480e-836e-735431de1e5e",
      "cell_type": "code",
      "source": "import pandas as pd\nimport numpy as np\nimport sklearn \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib\nimport warnings\nwarnings.filterwarnings('ignore')\nnp.random.seed(23)\nimport zipfile",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "2f38c78e-ddce-4bf9-8513-1c2c23a0b750",
      "cell_type": "markdown",
      "source": "## Wczytanie zbioru danych",
      "metadata": {}
    },
    {
      "id": "5ee268f5-8f93-438b-bbdf-db319188fba4",
      "cell_type": "code",
      "source": "zip_path = \"ecommerceDataset.csv.zip\"\n\nwith zipfile.ZipFile(zip_path, 'r') as z:\n    with z.open(\"ecommerceDataset.csv\") as file:\n        df = pd.read_csv(file, header=None)\n        \ndf.columns = ['category', 'description'] ",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "36fa07b1-a663-4d87-8621-6c2b8245eb37",
      "cell_type": "markdown",
      "source": "## Podstawowy stemming i lematyzacja",
      "metadata": {}
    },
    {
      "id": "ee305697-7dcf-4091-858f-3c7eaf023df6",
      "cell_type": "code",
      "source": "import nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\n\n# nltk.download('punkt')\n# nltk.download('stopwords')\n# nltk.download('wordnet')\n\nstop_words = set(stopwords.words('english'))\nstemmer = PorterStemmer()\nlemmatizer = WordNetLemmatizer()\n\n\ndef preprocess_text(text):\n    tokens = text.lower().split()  # zamiast word_tokenize\n    tokens = [word for word in tokens if word.isalpha()]\n    tokens = [word for word in tokens if word not in stop_words]\n    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n    return ' '.join(tokens)\n\ndf['clean_text'] = df['description'].apply(preprocess_text)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "4ede4b68-2b16-40d2-9451-e2bfec591303",
      "cell_type": "markdown",
      "source": "## TF-IDF na oczyszczonych tekstach",
      "metadata": {}
    },
    {
      "id": "0fd7e24c-8985-41cf-8b35-8177673b4b31",
      "cell_type": "code",
      "source": "tfidf = TfidfVectorizer(max_features=1000, ngram_range=(1, 2), stop_words='english')\nX_tfidf = tfidf.fit_transform(df['clean_text'])\n\n\n# Najważniejsze cechy wg tf-idf\nfeature_array = tfidf.get_feature_names_out()\ntfidf_sorting = X_tfidf.toarray().sum(axis=0).argsort()[::-1]\ntop_n = 20\ntop_features = feature_array[tfidf_sorting][:top_n]\n\n# Wykres\nplt.figure(figsize=(10,5))\nsns.barplot(x=top_features, y=X_tfidf.toarray().sum(axis=0)[tfidf_sorting][:top_n])\nplt.xticks(rotation=45)\nplt.title('Top cechy wg TF-IDF')\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "1335e496-b06e-48cc-8c69-880de15018d7",
      "cell_type": "markdown",
      "source": "## t-SNE",
      "metadata": {}
    },
    {
      "id": "43b649ef-b472-4825-9dbb-fbefb1f501a4",
      "cell_type": "code",
      "source": "# from sklearn.manifold import TSNE\n#\n# tsne = TSNE(n_components=2, perplexity=30, n_iter=1000, random_state=42)\n# X_tsne = tsne.fit_transform(X_tfidf.toarray())\n#\n# plt.figure(figsize=(10, 6))\n# plt.scatter(X_tsne[:, 0], X_tsne[:, 1], alpha=0.5, s=10)\n# plt.title(\"Redukcja wymiarów za pomocą t-SNE\")\n# plt.xlabel(\"Wymiar 1\")\n# plt.ylabel(\"Wymiar 2\")\n# plt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "3fb2ddf1-4e27-4a86-a8d7-b109b562b82d",
      "cell_type": "markdown",
      "source": "## Word2Vec",
      "metadata": {}
    },
    {
      "id": "3dd0e85c-b48b-4073-8ca1-157283c763a9",
      "cell_type": "code",
      "source": "from gensim.models import Word2Vec\n\n# tokenizacja\ntokenized = df['clean_text'].apply(lambda x: x.split())\n\n# wstępne trenowanie Word2Vec\nw2v_model = Word2Vec(sentences=tokenized, vector_size=100, window=5, min_count=2, workers=4)\n\ndef get_mean_embedding(doc):\n    words = [word for word in doc if word in w2v_model.wv]\n    if not words:\n        return np.zeros(100)\n    return np.mean(w2v_model.wv[words], axis=0)\n\nX_w2v = np.array([get_mean_embedding(doc) for doc in tokenized])\n\nprint(X_w2v.shape)\nprint(X_w2v[:5])",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}